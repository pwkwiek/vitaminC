{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as spio\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, BatchNormalization\n",
    "from matplotlib import image, pyplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingData():\n",
    "  for category in CATEGORIES:\n",
    "    path = os.path.join(path_test, category)\n",
    "    class_num = CATEGORIES.index(category)\n",
    "    for img in os.listdir(path):\n",
    "      img_array = cv2.imread(os.path.join(path,img))\n",
    "      new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "      training.append([new_array, CAT[class_num]])\n",
    "\n",
    "IMG_SIZE=400\n",
    "IMG_CHANNELS = 3\n",
    "CATEGORIES = [\"0\",\"20\",\"50\",\"80\",\"100\",\"150\",\"200\",\"300\",\"400\"]\n",
    "CAT = [0,4.54,11.31,18.05,22.52,33.63,44.64,66.37,87.72]\n",
    "\n",
    "training = []\n",
    "path_test = \"C:/Users/User/Desktop/Rivendell/bulb/const\"\n",
    "createTrainingData()\n",
    "path_test = \"C:/Users/User/Desktop/Rivendell/sun/const\"\n",
    "createTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_img, test_img = train_test_split(training, test_size=40) # 50\n",
    "train_img, val_img = train_test_split(train_val_img, test_size=40) # 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(training, dtype=object).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(train_img, dtype=object).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(val_img, dtype=object).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(test_img, dtype=object).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(train_img, dtype=object)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "h = 400\n",
    "im_samples = 512\n",
    "im_size = 50\n",
    "#im_rep = 11\n",
    "def create_images(im_names):\n",
    "  for i in range(np.array(im_names, dtype=object).shape[0]):\n",
    "    im = im_names[i][0]\n",
    "    conc = im_names[i][1]\n",
    "    for _ in range(im_samples):\n",
    "      ix = np.random.randint(low=a, high=h-im_size)\n",
    "      iy = np.random.randint(low=a, high=h-im_size)\n",
    "      yield (np.array(im[iy:iy+im_size, ix:ix+im_size, :]) / 128.)-1., np.array([conc]) # np.array([conc / 50.])\n",
    "\n",
    "def samples_count(im_names):\n",
    "  return len(im_names) * im_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count(val_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: create_images(train_img),\n",
    "    output_signature=(tf.TensorSpec((im_size, im_size, 3), dtype=\"float32\"), tf.TensorSpec((1), dtype=\"float32\"))\n",
    ")\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: create_images(val_img),\n",
    "    output_signature=(tf.TensorSpec((im_size, im_size, 3), dtype=\"float32\"), tf.TensorSpec((1), dtype=\"float32\"))\n",
    ")\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: create_images(test_img),\n",
    "    output_signature=(tf.TensorSpec((im_size, im_size, 3), dtype=\"float32\"), tf.TensorSpec((1), dtype=\"float32\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(y_true, y_pred):\n",
    "  y_true = y_true\n",
    "  y_pred = y_pred\n",
    "  return tf.keras.losses.mean_absolute_error(y_true, y_pred) + tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def loss_class(y_true, y_pred):\n",
    "  y_true_idx = tf.cast(y_true, tf.int64)\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(y_true_idx, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = keras.layers.Input(shape=(im_size, im_size, 3))  \n",
    "\n",
    "x = keras.layers.Conv2D(32, (1, 1), padding=\"valid\", kernel_regularizer=keras.regularizers.L2(0.0001))(inp)\n",
    "x = keras.layers.Activation(activation='gelu')(x)\n",
    "\n",
    "x = keras.layers.Conv2D(64, (1, 1), padding=\"valid\", kernel_regularizer=keras.regularizers.L2(0.0001))(x)\n",
    "x = keras.layers.Activation(activation='gelu')(x)\n",
    "x = keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(1, 1), padding='valid')(x)\n",
    "\n",
    "x = keras.layers.LayerNormalization()(x) \n",
    "\n",
    "x = keras.layers.Conv2D(64, (1, 1), padding=\"valid\", kernel_regularizer=keras.regularizers.L2(0.0001))(x)\n",
    "x = keras.layers.Activation(activation='gelu')(x)\n",
    "x = keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='valid')(x) \n",
    "\n",
    "#x = keras.layers.LayerNormalization()(x) \n",
    "x = keras.layers.GlobalMaxPool2D()(x) \n",
    "\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(150, activation='gelu', kernel_regularizer=keras.regularizers.L2(0.0001))(x) \n",
    "x = keras.layers.Dropout(0.1)(x) # MJ było 0.3\n",
    "x = keras.layers.Dense(50, activation='gelu', kernel_regularizer=keras.regularizers.L2(0.0001))(x)\n",
    "x = keras.layers.Dense(1, activation = 'linear')(x)\n",
    "\n",
    "model_o_m = keras.Model(inp, x)\n",
    "model_o_m.compile(loss=new_loss, optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.7, beta_2=0.9), metrics=[]) \n",
    "#metrics=['mae', 'mse', 'acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_o_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 30\n",
    "train_batches = train_dataset.repeat().shuffle(samples_count(train_img)).batch(bsize).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_batches = val_dataset.repeat().shuffle(samples_count(val_img)).batch(bsize).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model AA gener\n",
    "\n",
    "lr_drop = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    factor=0.1,\n",
    ")\n",
    "\n",
    "history = model_o_m.fit(\n",
    "    train_batches, \n",
    "    validation_data=val_batches, \n",
    "    steps_per_epoch=samples_count(train_img) // bsize,\n",
    "    validation_steps=samples_count(val_img) // bsize,\n",
    "    epochs=20,\n",
    "    callbacks=[lr_drop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylim(0, 50)\n",
    "plt.title('Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['val_loss', 'loss'], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "print('TRAIN SET');\n",
    "XX = []\n",
    "YY = []\n",
    "for xx, yy in train_dataset:\n",
    "    XX.append(xx)\n",
    "    YY.append(yy)\n",
    "XX = np.array(XX)\n",
    "YY = np.array(YY)\n",
    "\n",
    "\n",
    "pred_val = model_o_m.predict(XX)\n",
    "\n",
    "s1 = 512 # tyle obrazków z jednego zdjęcia\n",
    "s2 = 118 # 19\n",
    "YY1 = np.reshape(YY, (s1, s2, 1), order='F')\n",
    "YY2 = np.mean(YY1, axis=0)\n",
    "\n",
    "pred_val1 = np.reshape(pred_val, (s1, s2), order='F')\n",
    "pred_val2 = np.mean(pred_val1, axis=0)\n",
    "\n",
    "print('RMSEC:', np.sqrt(mean_squared_error(YY2[:, 0], pred_val2)))\n",
    "\n",
    "corr,_ = pearsonr(YY2[:,0],pred_val2)\n",
    "print('R^2 test:', corr**2)\n",
    "\n",
    "m, b = np.polyfit(YY[:,0], pred_val, 1)\n",
    "print('Slope:' , m) \n",
    "print('Intercept:', b) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(YY2[:, 0], pred_val2[:])\n",
    "ax.plot([YY2.min(), YY2.max()], [YY2.min(), YY2.max()], 'r-', lw=1)\n",
    "plt.xlabel('original [μg/ml]') \n",
    "plt.ylabel('predicted [μg/ml]')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('VALIDATION SET');\n",
    "XX = []\n",
    "YY = []\n",
    "for xx, yy in val_dataset:\n",
    "    XX.append(xx)\n",
    "    YY.append(yy)\n",
    "XX = np.array(XX)\n",
    "YY = np.array(YY)\n",
    "\n",
    "pred_val = model_o_m.predict(XX)\n",
    "\n",
    "s1 = 512\n",
    "s2 = 40 #30\n",
    "YY1 = np.reshape(YY, (s1, s2, 1), order='F')\n",
    "YY2 = np.mean(YY1, axis=0)\n",
    "\n",
    "pred_val1 = np.reshape(pred_val, (s1, s2), order='F')\n",
    "pred_val2 = np.mean(pred_val1, axis=0)\n",
    "\n",
    "print('RMSECV:', np.sqrt(mean_squared_error(YY2[:, 0], pred_val2)))\n",
    "corr,_ = pearsonr(YY2[:,0],pred_val2)\n",
    "print('R^2 test:', corr**2)\n",
    "\n",
    "m, b = np.polyfit(YY[:,0], pred_val, 1)\n",
    "print('Slope:' , m) \n",
    "print('Intercept:', b) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(YY2[:, 0], pred_val2[:])\n",
    "ax.plot([YY2.min(), YY2.max()], [YY2.min(), YY2.max()], 'r-', lw=1)\n",
    "plt.xlabel('original [μg/ml]') \n",
    "plt.ylabel('predicted [μg/ml]')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TEST SET');\n",
    "XXt = []\n",
    "YYt = []\n",
    "for xx, yy in test_dataset:\n",
    "    XXt.append(xx)\n",
    "    YYt.append(yy)\n",
    "XXt = np.array(XXt)\n",
    "YYt = np.array(YYt)\n",
    "\n",
    "pred_test= model_o_m.predict(XXt)\n",
    "\n",
    "s1 = 512\n",
    "s3 = 40 # 50\n",
    "YYt1 = np.reshape(YYt, (s1, s3, 1), order='F')\n",
    "YYt2 = np.mean(YYt1, axis=0)\n",
    "\n",
    "pred_test1 = np.reshape(pred_test, (s1, s3), order='F')\n",
    "pred_test2 = np.mean(pred_test1, axis=0)\n",
    "\n",
    "print('RMSEP:', np.sqrt(mean_squared_error(YYt2[:, 0], pred_test2)))\n",
    "corr,_ = pearsonr(YYt2[:,0],pred_test2)\n",
    "print('R^2 test:', corr**2)\n",
    "\n",
    "m, b = np.polyfit(YYt[:,0], pred_test, 1)\n",
    "print('Slope:' , m) \n",
    "print('Intercept:', b) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(YYt2[:, 0], pred_test2)\n",
    "ax.plot([YYt2.min(), YYt2.max()], [YYt2.min(), YYt2.max()], 'r-', lw=1)\n",
    "plt.xlabel('original [μg/ml]') \n",
    "plt.ylabel('predicted [μg/ml]')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Miod01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
